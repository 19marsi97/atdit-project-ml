{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G Neural Networks Overfitting\n",
    "_6 points_\n",
    "\n",
    "- Train a neural net and prevent overfitting by regularization. \n",
    "- You can use any combination of regularizers we saw in class.\n",
    "- Use the train and test splits in the data do evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change batch_size and epochs for fine tuning\n",
    "# image_classes MUST remain at 10!!!\n",
    "\n",
    "batch_size = 128\n",
    "image_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = to_categorical(y_train, image_classes)\n",
    "y_test = to_categorical(y_test, image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "decay = 1e-4\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), activation=\"softmax\", input_shape=(32, 32, 3), kernel_regularizer=regularizers.l2(decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"softmax\", kernel_regularizer=regularizers.l2(decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation=\"softmax\", kernel_regularizer=regularizers.l2(decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation=\"softmax\", kernel_regularizer=regularizers.l2(decay)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation=\"softmax\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(image_classes, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 32)        2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4, 4, 128)         16512     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 142,058\n",
      "Trainable params: 141,546\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compilation\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "50000/50000 [==============================] - 228s 5ms/step - loss: 1.9437 - acc: 0.2752 - val_loss: 1.8240 - val_acc: 0.3172\n",
      "Epoch 2/20\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 1.7256 - acc: 0.3711 - val_loss: 1.7131 - val_acc: 0.3763\n",
      "Epoch 3/20\n",
      "50000/50000 [==============================] - 232s 5ms/step - loss: 1.6105 - acc: 0.4191 - val_loss: 1.6838 - val_acc: 0.4018\n",
      "Epoch 4/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.5607 - acc: 0.4402 - val_loss: 1.7976 - val_acc: 0.3838\n",
      "Epoch 5/20\n",
      "50000/50000 [==============================] - 232s 5ms/step - loss: 1.5013 - acc: 0.4673 - val_loss: 1.4630 - val_acc: 0.4794\n",
      "Epoch 6/20\n",
      "50000/50000 [==============================] - 229s 5ms/step - loss: 1.4564 - acc: 0.4861 - val_loss: 1.5996 - val_acc: 0.4493\n",
      "Epoch 7/20\n",
      "50000/50000 [==============================] - 229s 5ms/step - loss: 1.4188 - acc: 0.4994 - val_loss: 1.4875 - val_acc: 0.4674\n",
      "Epoch 8/20\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 1.3766 - acc: 0.5159 - val_loss: 1.5544 - val_acc: 0.4726\n",
      "Epoch 9/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.3555 - acc: 0.5261 - val_loss: 1.3698 - val_acc: 0.5236\n",
      "Epoch 10/20\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 1.3151 - acc: 0.5417 - val_loss: 1.6472 - val_acc: 0.4674\n",
      "Epoch 11/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.2985 - acc: 0.5500 - val_loss: 1.4680 - val_acc: 0.4941\n",
      "Epoch 12/20\n",
      "50000/50000 [==============================] - 232s 5ms/step - loss: 1.2553 - acc: 0.5652 - val_loss: 1.3160 - val_acc: 0.5456\n",
      "Epoch 13/20\n",
      "50000/50000 [==============================] - 232s 5ms/step - loss: 1.2503 - acc: 0.5662 - val_loss: 1.2607 - val_acc: 0.5666\n",
      "Epoch 14/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.2319 - acc: 0.5770 - val_loss: 1.4565 - val_acc: 0.5103\n",
      "Epoch 15/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.2344 - acc: 0.5749 - val_loss: 1.2549 - val_acc: 0.5629\n",
      "Epoch 16/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.2006 - acc: 0.5867 - val_loss: 1.1454 - val_acc: 0.6051\n",
      "Epoch 17/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.1833 - acc: 0.5935 - val_loss: 1.1313 - val_acc: 0.6121\n",
      "Epoch 18/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.1647 - acc: 0.6000 - val_loss: 1.2235 - val_acc: 0.5839\n",
      "Epoch 19/20\n",
      "50000/50000 [==============================] - 230s 5ms/step - loss: 1.1581 - acc: 0.6052 - val_loss: 1.2959 - val_acc: 0.5554\n",
      "Epoch 20/20\n",
      "50000/50000 [==============================] - 231s 5ms/step - loss: 1.1549 - acc: 0.6059 - val_loss: 1.1693 - val_acc: 0.5982\n"
     ]
    }
   ],
   "source": [
    "train_size = 50000\n",
    "test_size = 10000\n",
    "if (y_test.shape == (10000, 10)):\n",
    "    model.fit(x_train[:train_size], y_train[:train_size],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=True,\n",
    "          validation_data=(x_test[:test_size], y_test[:test_size]))\n",
    "else:\n",
    "    raise AttributeError(\"y_test.shape must be (10000, 10) but is {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.1692674964904786\n",
      "Test accuracy: 0.5982\n"
     ]
    }
   ],
   "source": [
    "#score = model.evaluate(x_test[test_size:], y_test[test_size:], verbose=0)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Answer\n",
    "\n",
    "In this version we modified the model and made it a little more complex. Adding additional layers and doing the process of Convolution, Pooling, Dropout twice, we thought it give us better results. This is not the case directly; however, the difference we got between the accuracies in the training epochs and at the evaluation is quite outstanding. Both values only differ by about 0.7 % which is very good. Even though the final accuracy is \"only\" at 59%, we believe that this implementation shows that multiple changes to the model regarding regularization can improve it and prevent overfitting, but they do not always lead to a better model (in accuracy terms) or a faster runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
