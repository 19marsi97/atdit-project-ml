{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G Logistic regression MNIST\n",
    "_4 points_\n",
    "\n",
    "Evalute logistic regression as B  on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape(x_train.shape[0], x_train.shape[1] * x_train.shape[2])\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\d070700\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_regression = LogisticRegression(n_jobs=-1, max_iter=500) # use all processor cores\n",
    "log_regression.fit(x_train_flat[:2000], y_train[:2000]) # subsample because it is a very slow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = log_regression.predict(x_test_flat[:500]) # again, subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test[:500], predictions, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\d070700\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='ovr', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='saga', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because the normal solver is not optimized for multiclass scenarios and it is not able to run on multiple cores,\n",
    "# we tried the \"saga\" solver, which is better suited for multiclass problems\n",
    "\n",
    "log_regression_saga = LogisticRegression(solver=\"saga\", n_jobs=-1, max_iter=500)\n",
    "log_regression_saga.fit(x_train_flat[:2000], y_train[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_saga = log_regression_saga.predict(x_test_flat[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.842"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test[:500], prediction_saga, average=\"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\d070700\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9184"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above run was only to show the comparison for the same datasets. Because the saga solver runs better,\n",
    "# we tried it on the whole set\n",
    "log_regression_saga.fit(x_train_flat, y_train)\n",
    "prediction_saga = log_regression_saga.predict(x_test_flat)\n",
    "f1_score(y_test, prediction_saga, average=\"micro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "Logistic regression can be used to identify dicrete labels (like categories) in datasets. The given dataset consists of 10 different categories.\n",
    "Because the default solver for the logistic regression is optimized for binary problems (two classes), it did not perform too good on the dataset. In addition, it is quite slow because this solver does not allow to be split to all CPU-cores. By subsampling 2000 training data and testing with 500 test data examples one achieves an f1_score of 82.4%. (Again, using the f1_score to combine precision and recall and taking into account true negatives and false positives).<br>\n",
    "We tested out another solver, one that is recommended by the documentation for lage datasets and capable of multiclass problems. With the same subsample, the \"saga\" solver is able to achieve a f1_score of 84.2%.\n",
    "If you invest more time and use the full dataset on the saga solver, one can achieve 91.84% in f1-score which is a very good result.\n",
    "\n",
    "Logistic regression is an acceptable model for this dataset, the values are quite accurate but the KNN results with >96% were better. On the other hand, it performed far better than kmeans which had errors all over the place.\n",
    "Nevertheless, for an accurate prediction with as little mistakes as possible, this model is not recommendable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
